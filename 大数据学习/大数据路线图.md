# 大数据学习路线图

**一份清晰、系统、针对性的“大数据开发学习路线图”**，**匹配现阶段的能力**，**对接实际求职需求（秋招为主）**

## **大数据开发学习路线图（秋招导向）**

> **阶段定位**：已完成 LeetCode 刷题、Python SQL 基础、Hive/HDFS 部署等初步探索，下一步需要的是**系统化补强+项目实战+面试能力构建**。

### **一、核心技能地图**

| 分类         | 技术栈/工具                                 | 推荐掌握深度                            | 备注                            |
| ---------- | -------------------------------------- | --------------------------------- | ----------------------------- |
| **基础语言**   | Python、SQL                             | 熟练掌握                              | 面试高频、项目主力语言                   |
| **数据库**    | MySQL、ClickHouse                       | 掌握原理+优化+索引使用                      | SQL 实战场景多，ClickHouse作为亮点经验可保留 |
| **分布式计算**  | Hadoop（HDFS/YARN/MapReduce）、Hive、Spark | Hadoop、Hive要熟；Spark至少掌握核心原理与使用场景  | Hive需具备手写SQL能力；Spark是秋招必问     |
| **调度与采集**  | Flume、Kafka、Sqoop                      | Kafka需重点掌握；Flume/Sqoop了解原理、掌握基本配置 | Kafka高频考点                     |
| **数据仓库理论** | ODS-DWD-DWS-ADS、维度建模、分层、宽表等            | 深度理解 + 会画图 + 能讲解                  | 数仓原理+分层+建模是数据开发的灵魂            |
| **数据可视化**  | PowerBI / Tableau / Echarts            | 简单使用                              | 项目展示更有说服力                     |
| **脚本工具**   | Shell                                  | 熟练掌握基本命令 + 项目中实际使用                | 常用于调度与运维环节                    |
| **版本管理**   | Git、Github                             | 熟练使用                              | 为简历加分（托管项目）                   |


### **二、系统化阶段路线（建议配合五天一轮周期）**

| 阶段                         | 时间建议    | 学习内容                                                               | 输出成果                                            |
| -------------------------- | ------- | ------------------------------------------------------------------ | ----------------------------------------------- |
| **阶段一：夯实基础**               | 5\~7天   | Linux常用命令，Shell、MySQL优化、SQL八股熟记                              | 复习笔记+Shell练习+MySQL优化清单                          |
| **阶段二：掌握大数据核心组件**          | 10天     | Hadoop三大组件 + Hive部署配置 + MapReduce使用 + Spark基础（RDD/Stage/DataFrame） | 成功跑通案例，整理配置手册、总结架构原理                            |
| **阶段三：数据采集/传输/同步**         | 7天      | Kafka部署使用、Flume基本用法、Sqoop使用（MySQL→HDFS）                            | 数据流转Demo项目，写一篇数据流通笔记                            |
| **阶段四：数仓分层 + 维度建模**        | 7\~10天  | 分层建模理论（ODS\~ADS）、宽表、雪花/星型、常见主题拆解方法                                 | 构建你离线项目的“数仓版”，画出分层图、建模图                         |
| **阶段五：Spark进阶 + 项目实战**     | 10\~15天 | Spark核心机制（RDD、DAG、数据倾斜、调优）、ETL流程、Shell调度                           | 项目级别内容，完成一个 Spark + Hive 项目（可复用 ClickHouse 旧项目） |
| **阶段六：面试高频 + 八股精记 + 项目精修** | 滚动推进    | 八股文记忆背诵，SQL手撕巩固，Spark调优问题，MySQL索引设计等                               | 建立面试文档库 + 模拟面试材料 + GitHub更新项目文档                 |



### **三、建议使用资料（结合视频+文档+实践）**

| 类别   | 推荐资料                                           |
| ---- | ---------------------------------------------- |
| 视频路线 | 尚硅谷大数据全套（优先）、狂神、黑马、极客时间（spark/kafka专栏）         |
| 文档类  | GitHub大数据八股文合集（你已找到）+ Hadoop/Spark/Hive 官方文档查阅 |
| 练习类  | LeetCode SQL专栏、开源项目实战视频                        |
| 补充   | 学习笔记每日总结+项目拆解分析输出+定期回顾                         |



### **四、每周结构建议（可灵活套用）**

| 周期     | 内容安排                   | 备注      |
| ------ | ---------------------- | ------- |
| Day1-2 | 视频+实操主线推进（如Kafka+Hive） | 保证代码同步敲 |
| Day3   | 八股文记忆 + SQL实战 + 项目代码整理 | 巩固性日    |
| Day4   | Spark学习/数仓理论/项目补充      | 深度日     |
| Day5   | 面试准备+复盘+GitHub文档更新     | 输出性日    |


### **五、最后建议**

* **不要“学了就过”**，所有内容都要带着“我能讲出来”再离开。
* **项目不是做完，是要做“熟”**，每个细节你都能解释出来。
* **不怕重复、只怕“假掌握”**，视频可以反复看，代码可以反复敲。
* **八股文熟背熟练手撕 SQL + Spark 面试题，就是你的大招。**























---

---

---

# 大数据入门



阶段计划：
| 技术              | 需不需要看    | 原因             |
| --------------- | -------- | -------------- |
| **Linux**       | **必须**   | 所有部署与操作的根基     |
| **HDFS**        | **必须**   | 所有大数据存储的底层     |
| **YARN**        | 可选了解     | 调度系统，非主力操作组件   |
| **Hive**        | **必须**   | 数仓主角，查询引擎      |
| **Sqoop/Flume** | **了解即可** | 项目补充，非主力考点     |
| **Kafka**       | **建议学**  | 实时流数据处理核心，面试必问 |
| **Spark**       | **必须**   | 计算引擎的主力选手，考频极高 |


---

## Linux基础

| 类别          | 关键词                                                    | 建议内容               |
| ----------- | ------------------------------------------------------ | ------------------ |
| **文件系统操作**  | `ls` `cd` `rm` `mv` `cp` `touch` `mkdir` `find` `grep` | 最基本的生存能力           |
| **权限和用户管理** | `chmod` `chown` `sudo` `passwd` `su` `useradd`         | 学会读懂 `drwxr-xr-x`  |
| **进程与网络管理** | `ps` `top` `netstat` `ping` `lsof`                     | 日常排查进程问题、端口冲突      |
| **压缩打包**    | `tar` `zip` `unzip` `gzip`                             | 部署必备               |
| **文本处理三剑客** | `awk` `sed` `cut`                                      | 数据处理神器（后期超有用）      |
| **脚本与调度**   | `for/while` `if` `cron` `bash`                         | 自动化调度 Shell 脚本     |
| **日志定位**    | `tail -f` `cat` `less` `grep`                          | 分析 Hive 报错、HDFS 状态 |


- Linux内核、虚拟机、查看虚拟机网卡状态Win+R输入`ncpa.cpl`查看是否有VMnet1和VMnet8；
- Linux命令行效率远大于图形化界面；`ifconfig`显示当前系统所有的网络接口的详细参数；Linux中路径使用`/`；
- Linux命令基础格式：`command [-options：可选，控制命令执行细节] [parameter：可选，命令的参数，命令指向的目标]`
- `ls`平铺当前工作目录下内容；`ls [-a(看所有文件，包括隐藏文件) -l(竖着输出) -h(以易于阅读的形式现实文件大小)] [指定目录]`
- `cd [指定路径]`：切换工作目录（转到指定目录或者返回最初HOME）；`pwd`查看当前目录；
- 相对路径与绝对路径：`cd`：`cd ..`退一级，`cd ../..`退两级，`~`表示HOME目录，`.`当前目录；
- `mkdir [-p（多级创建）] Linux路径`；
- `touch Linux路径`：创建文件；`cat`(查看全部内容)、`more`(查看部分内容，空格翻页，q退出)；
- `cp [-r（复制文件夹需要加上）] 参数1 参数2`，`mv 参数1 参数2`（可以改名字）；
- `rm [-r(用于删除文件夹) -f(强制删除)] 参数1 参数2 ... 参数N`；`*`通配符，`test*`以test开头的，`*test`以test结尾的， `*test*`只要是包含test的；
- `which 要查找的命令`；
- find按照文件名查找文件：`find 起始命令 -name "被查找文件名"`，find+通配符，find按照文件大小查找文件：`find 起始路径 -size +|-n[kMG]`,+、-表示大于小于，n表示大小数字，kMG分别表示kb、MB、GB
- `grep [-n] 关键字 文件路径`：从文件中通过关键字过滤文件行；
- `wc [-c -m -l -w] 文件路径`：-c统计bytes数量，-m统计字符数量，-l统计行数，-w统计单词数量，什么都不加的时候就是`行数词数字节数`；
- `命令1 | 命令2` 管道符，将左边的结果作为右边的输入；例如`cat test.txt | grep itheima`
- `echo "输出的内容"`：在命令行内输出指定内容（prin函数）；“反引号 ` ”，反引号(飘号)内的内容会被当做命令执行；
- 重定向符：`>`将左侧命令的结果**覆盖**写入到符号右侧指定的文件中，`>>`将左侧命令的结果**追加**写入到符号右侧指定的文件中（下一行）；
- `tail [-f -num] Linux路径`，-f表示持续跟踪，-num表示查看尾部多少行默认10：用于查看文件尾部的内容，跟踪文件的最新更改。
- vim编辑器：命令模式（很多快捷键）、输入模式、底线命令模式（:进入）
- `su [-(环境变量，最好添加)] [用户名]`：用于切换账户，扩减权限；`sudo 其他命令`：临时以root身份执行，并非所有用户有用sudo权限，需要为普通用户配置sudo认证；
- 用户与用户组：`groupadd 用户组名`、`groupdel 用户组名`分别为添加删除用户组；`useradd[-g -d]用户名`、`userdel[-r]用户名`、`id[]`、`usermod -aG 用户组 用户名 `：创建用户、删除用户、查看用户所属组、修改用户所属组；`getent passwd/group`查看系统中全部用户信息/全部用户组信息，表示`[用户名][密码（x）][用户ID][组ID][描述信息（无用）][HOME目录][执行终端（默认bash）]`。
- 查看权限控制信息：
  - <img src="https://github.com/user-attachments/assets/7f429978-4c68-4b30-9849-5d1774f0e89d" width="600">
  - r、w、x：代表读权限(eg:ls)、写权限、执行权限(eg:cd)
- `chmod [-R] 权限 文件或文件夹`：修改文件文件夹的权限信息，-R对文件夹内全部内容应用同样的操作；
  - 0：---、1：--x、2：-w-、3：-wx、4：r--、5：r-x、6：rw-、7：rwx（r=4、w=2、x=1）
- `chown [-R] [用户][:][用户组] 文件或文件夹`：修改文件文件夹的所属用户和用户组（此命令限于root用户）
- `Ctrl+c`强制退出、`Ctrl+d`退出或者登出、`history`历史命令搜索(!倒着搜索最近的命令)、`Ctrl+r`输入内容匹配历史命令
- `yum [-y] [install | remove | search] 软件名称`
- `systemctl start | stop | status | enable | disable 服务名`
- `ln -s 参数1 参数2`；创建软连接，类似于快捷方式，比如`ln -s /etc/yum.conf ~/yum.conf`
- `date [-d] [+格式化字符串]`：可以在命令行中查看系统的时间，-d一般用于日期计算；修改Linux时区；
- IP地址：`ifconfig`来查看，`127.0.0.1`代指本机，`0.0.0.0`代指本机·确定绑定关系·允许任意IP访问；主机名`hostname`，修改`hostnamectl set-hostname 新的名字`；域名解析，替代直接记忆IP地址；
- 网络请求和下载：`ping [-c num] ip或主机名`，检查指定的网络服务器是否是可联通状态；`wget [-b] url`，在命令行内下载网络文件，-b后台下载；`curl [-O] url`，发送http网络请求用于下载文件获取信息等，-O是否下载；
- Linux超大号小区，6万多端口，`公认端口：1~1023；注册端口：1024~49151；动态端口：49152~65535`；
- 查看端口占用：`nmap 被查看的IP地址`，`netstat -anp|grep 端口号`；
- 进程管理：`ps [-e -f]`，查看Linux系统中的进程信息（-e显示全部进程，-f展示全部信息，一般-ef）；`kill [-9] 进程ID`，关闭进程（-9表示强制关闭）；
- 查看系统资源占用：`top`直接进入，默认5秒刷新一次，Ctrl+c或者q退出；有很多参数可选：-p、-d、-c、-n、-b、-i、-u；top支持交互式选型（多，各种按键）；
  - <img src="https://github.com/user-attachments/assets/85648e1a-686b-48a8-b916-39c8e48c75c8" width="300">
  - <img src="https://github.com/user-attachments/assets/eba4fb7b-232b-4278-af7d-76743f3cd1f4" width="300">
  - <img src="https://github.com/user-attachments/assets/d7d1288d-2016-413b-8c32-2b66141c7114" width="300">
  - <img src="https://github.com/user-attachments/assets/df2f7876-2fa5-4915-bc09-11db7556a254" width="300">

- 磁盘信息监控：`df [-h]`查看磁盘使用情况，-h更人性化展示；`iostat [-x][num1][num2]`，-x显示更多信息，num1刷新间隔，num2刷新次数；
  - <img src="https://github.com/user-attachments/assets/210481cd-b54a-4675-988a-450d97ef88aa" width="300">
- 网络状态监控：`sar -n DEV num1 num2`，查看网络的相关统计信息
- 环境变量：`env`直接查看Linux中的环境变量（key=value键值对形式的）；`PATH`记录了系统执行任何命令的搜索路径（：隔开）；`$符号`相当于`${key}=value`函数，输出value；
- 自行设置环境变量：临时设置`export 变量名=变量值`；永久生效需要更改~/.bashrc文件（当前用户），/etc/profile文件（所有用户）；
- 上传、下载：图形化（FinalShell最下方）；`rz：上传（很慢，建议拖拽） sz：下载`；
- 压缩与解压：`tar [-c -v -x -f -z -C] 参数1 参数2 ... 参数N`，`.tar 简单封装 .gz 极大压缩`；常用组合`-cvf tar模式`、`-zcvf gz模式`
  
```
-c，创建压缩文件，用于压缩模式；
-v ，显示压缩、解压过程，用于查看进度；
-x ，解压模式；
-f ，要创建的文件，或要解压的文件（-f 选项必须在所有选项中位置处于最后一个）；
-z ， gzip 模式，不使用 -z 就是普通的 tarball 格式；
-C ，选择解压的目的地，用于解压模式
```

- `zip [-r] 参数1 参数2 ... 参数N`：压缩文件为zip压缩包；`unzip [-d] 参数`：解压zip压缩包（-d指定解压位置）；






















---

>Hive、Hadoop

> 1. 简答各大组件的定位和特点
>- **Hadoop HDFS**：分布式文件系统，负责海量数据的高可靠、高吞吐存储，把大文件切分成块后分布在不同机器上。  
>- **Hadoop MapReduce**：分布式离线计算框架，基于"Map-Reduce"模型进行大规模批处理，可靠但延迟高。  
>- **Hadoop YARN**：资源调度与管理平台，统一调度管理集群资源，为上层作业（如MR、Spark）提供运行环境。  
>- **Hive**：基于Hadoop的开源数仓工具，将SQL转化为MapReduce等任务，适合大规模离线分析，易用但查询延迟高。  
>- **Flink**：流批一体的分布式计算框架，擅长低延迟、实时数据处理，支持有状态计算和复杂事件处理。  
>- **Spark**：基于内存的大数据计算引擎，比MR快很多，支持批处理、流处理、机器学习等一站式计算。  
>- **Kafka**：高吞吐量的分布式消息队列系统，用于实时数据采集、传输、流处理的核心中间件。  
>- **ClickHouse**：高性能列式数据库，擅长海量数据的分析型查询，OLAP场景中支持亚秒级响应。  
>- **Doris**：MPP（Massively Parallel Processing）架构的分析型数据库，支持高并发实时查询，且兼容MySQL生态。


>
> 2. 明确掌握 HDFS 的关键点
>
>- **为什么要用分布式文件系统？**  
>  因为单台机器无法存储和处理海量数据，分布式系统可以通过多台机器协作存储和容错，提高扩展性与可靠性。
>
>- **HDFS的存储机制**  
>  文件被切分成固定大小的**Block块**（默认128MB），每个块复制到多个节点（默认副本数3）以保证容错。
>
>- **NameNode 和 DataNode职责**  
>  - **NameNode**：负责存储**元数据**（文件目录结构、块位置映射），是HDFS的"大脑"。  
>  - **DataNode**：负责实际存储数据块，并定期向NameNode汇报存储状态。
>
>- **HDFS读写流程**  
>  - **读流程**：客户端向NameNode请求文件块位置信息，之后直接从对应的DataNode读取数据。  
>  - **写流程**：客户端向NameNode申请写入文件，然后按副本策略将数据块顺序写入到多个DataNode上，保证数据冗余。


>
>3. Hadoop存在的最大意义是什么？（简写版）
>
>Hadoop最大的意义在于**让普通硬件也能以可靠、可扩展、低成本的方式处理和存储海量数据**，为大数据时代提供了基础设施标准，推动了整个大数据生态系统的发展。



---

## 基础知识

#### 1. 大数据的核心工作：
 
- 存储：妥善保存海量待处理数据
- 计算：完成海量数据的价值挖掘
- 传输：协助各个环节的数据传输
 
#### 2. 大数据软件生态：
 
- 存储：Apache Hadoop HDFS、Apache HBase、Apache Kudu、云平台
- 计算：Apache Hadoop MapReduce、Apache Spark、Apache Flink
- 传输：Apache Kafka、Apache Pulsar、Apache Flume、Apache Sqoop


#### 3.关于Apache的补充：

- **Apache软件基金会（Apache Software Foundation，简称ASF）** 是全球最大的开源软件基金会，于1999年由Apache Group成员组建，是一家在美国注册成立的501(c)(3)非营利公共慈善组织。它致力于为开放、协作的软件开发项目提供硬件、通信和业务基础设施等支持，创建独立法人实体以接受资源捐赠并确保用于公共利益，同时为个体志愿者提供法律保护，避免针对基金会项目的诉讼，并防止“Apache”品牌软件被滥用。ASF几乎完全由志愿者运营，监管着数百个项目，涵盖Apache HTTP服务器、Apache Tomcat、Apache Hadoop等知名项目，所发行软件遵循Apache许可证。其以共识为导向的开放式开发流程，在人工智能、大数据、云计算、边缘计算等领域推动了突破性创新，深刻影响现代计算的各个方面，使全球数十亿人受益，是开源领域极具影响力的组织，持续为开源技术发展与应用贡献力量。
- **Apache（Apache HTTP Server）** 是Apache软件基金会旗下一款开源的网页服务器，也是世界使用排名第一的Web服务器软件，能在几乎所有主流计算机平台上运行，以跨平台、高安全性、简单快速且性能稳定著称。其源于NCSA httpd服务器，经多次修改优化，因是自由软件，不断有开发者为其增添新功能、特性及修复缺陷，名称既取自“a patchy server”（充满补丁的服务器）的谐音，也与北美印第安部落名称呼应。它支持最新的HTTP/1.1通信协议、基于IP和域名的虚拟主机、多种HTTP认证、SSL技术、FastCGI等，还可通过简单API扩充，将Perl/Python等解释器编译到服务器中，兼具代理服务器功能。凭借源代码开放、跨平台支持及出色的可移植性，Apache被广泛应用，像Amazon、Yahoo!等知名网站都曾采用，成为最流行的Web服务器端软件之一。

4.为什么需要分布式存储？
 
- 数据量太大，单机存储能力有上限，需要靠数量来解决问题
- 数量的提升带来的是网络传输、磁盘读写、CPU、内存等各方面的综合提升。分布式组合在一起可以达到1+1>2的效果

## Hadoop

#### 1.HDFS

**分布式文件文件系统**

1.存储原理，副本存储，冗余存储

2.副本数量默认值3；FSCK命令：

```
hadoop fs -setrep [-R] 2 path    //指定path会被修改为2个副本存储
```

每Block块256m，可修改。EditLog流水账合并为最终结果FsImage（定期的合并（一小时或者一百万次操作），定期六十秒检查），合并由SNN来操作。

3.写读流程

写入流程：



```
1. 客户端向NameNode发起请求
2. NameNode审核权限、剩余空间后，满足条件允许写入，并告知客户端写入的DataNode地址
3. 客户端向指定的DataNode发送数据包
4. 被写入数据的DataNode同时完成数据副本的复制工作，将其接收的数据分发给其它DataNode
5. 如上图，DataNode1复制给DataNode2，然后基于DataNode2复制给Datanode3和DataNode4
6. 写入完成客户端通知NameNode，NameNode做元数据记录工作
```

读入流程：




```
1、Client通过Distributed FileSystem模块向NN请求下载文件，NN通过查询元数据，找到文件的DN地址
2、Client挑选一台最近的DN，请求读取数据
3、DN开始传输数据给Client(从磁盘里读取数据输入流，以packet为单位做校验)
4、Client以packet为单位接收，先在本地缓存，然后写入目标文件
```

#### 2.MapReduce与YARN
分布式计算引擎与资源调度器

```
1.分布式计算中，分散-汇总计算模式（MapReduce），中心调度-步骤执行（Spark、Flink），
2.对于资源的利用，有规划、有管理的调度资源使用，是效率最高的方式
3.YARN基础架构：主从架构ResourceManager、NodeManager
    ResourceManager：整个集群的资源调度者，负责协调调度各个程序所需的资源。
    NodeManager：单个服务器的资源调度者，负责调度单个服务器上的资源提供给应用程序使用。
    容器Container：NodeManage预先占用资源，再把资源提供给程序。程序无法突破容器资源限制。
4.YARN辅助架构：代理服务器和历史服务器
```

## Hive

### 分区表及语法
- 可以选择字段作为表分区；分区其实就是HDFS上的不同文件夹；提高特定场景下Hive的操作性能；
- 分区列在在```PARTITIONDE BY```中定义
```sql
CREATE TABLE TABLENAME(...) PARTITIONED BY (分区列,类型列,...)
ROW FROMAT DELIMITED FIELDS TERMINATED BY '';
```
### 分桶表及语法
- 分桶和分区一样，也是一种通过改变表的存储模式，从而完成对表优化的一种调优方式；
- 分区是将表拆分到**不同子文件夹**中进行存储，而分桶是将表拆分到**固定数量的不同文件**中进行存储。
  
```sql
##开启分桶优化（自动匹配reduce task数量和桶的数量一致）
SET hive.enforce.buketing=TRUE;
##创建分桶表
CREATE TABLE course (c_id string,c_name string,t_id string) CLUSTERED BY (c_id)
INTO 3 bucket ROW FORMAT delimited fields terminated BY '\t';
```
















## MySQL

### SQL基础语句
在 MySQL 中，DDL、DML、DQL、DCL 是几类不同功能的 SQL 语句，具体如下：
 
1. DDL（Data Definition Language，数据定义语言）
 
用于定义数据库的逻辑结构（元数据），如创建、修改、删除数据库对象（数据库、表、索引、视图等）。常用命令包括：
 
-  CREATE ：创建数据库或表等对象，例如  CREATE TABLE table_name (col1 data_type, ...) 。
-  ALTER ：修改表结构，如添加、删除列或修改列类型，例如  ALTER TABLE table_name ADD column_name data_type 。
-  DROP ：删除数据库对象，如  DROP TABLE table_name 。
-  TRUNCATE ：清空表中所有数据并释放空间，如  TRUNCATE TABLE table_name 。
-  RENAME ：重命名对象（如表）。
 
2. DML（Data Manipulation Language，数据操作语言）
 
用于操作表中的数据，实现数据的增、删、改等操作。常用命令包括：
 
-  INSERT ：插入数据，如  INSERT INTO table_name (col1, col2) VALUES (val1, val2) 。
-  UPDATE ：更新数据，如  UPDATE table_name SET col1 = new_val WHERE condition 。
-  DELETE ：删除数据，如  DELETE FROM table_name WHERE condition 。
-  REPLACE ：替换数据（若主键或唯一索引冲突，先删除旧行再插入新行）。
-  LOAD DATA ：高速从文本文件导入数据到表中。
 
3. DQL（Data Query Language，数据查询语言）
 
用于从数据库中查询数据，最典型的是  SELECT  语句。可配合子句实现复杂查询：
 
-  SELECT ：检索数据，如  SELECT col1, col2 FROM table_name WHERE condition GROUP BY col1 HAVING group_condition ORDER BY col2 LIMIT start, count 。
 
4. DCL（Data Control Language，数据控制语言）
 
用于控制数据库的访问权限，管理用户对数据库的操作权限。常用命令包括：
 
-  GRANT ：赋予用户权限，如  GRANT SELECT, INSERT ON database_name.* TO 'user'@'host' 。
-  REVOKE ：收回用户权限，如  REVOKE INSERT ON database_name.* FROM 'user'@'host' 。
 
通过这四类语句，可分别实现数据库结构定义、数据操作、数据查询及权限控制，覆盖数据库管理的主要场景。




